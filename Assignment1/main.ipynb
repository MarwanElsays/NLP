{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset,concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('sst', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 0.625,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class(value):\n",
    "    return np.digitize(value, bins=[0.2, 0.4, 0.6, 0.8], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapped = dataset.map(lambda example: {'label': map_class(example['label'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training set\n",
    "train_data = concatenate_datasets([dataset_mapped['train'], dataset_mapped['validation']])\n",
    "train_labels = np.array(train_data['label'],dtype=np.int8)\n",
    "\n",
    "# Access the test set\n",
    "test_data = dataset_mapped['test']\n",
    "test_labels = np.array(test_data['label'],dtype=np.int8)\n",
    "\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 3.0,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = test_data[10]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Na√Øve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D,num_classes):\n",
    "     \n",
    "    n_doc = len(D)  # Total number of documents/examples\n",
    "    cls_prior = np.zeros(num_classes)  # Initialize class prior probabilities\n",
    "    vocab = set()  # To store the vocabulary\n",
    "    cls_word_cnt = np.array([{} for _ in range(num_classes)])  # Initialize class word counts\n",
    "\n",
    "    # Loop through each example in the dataset\n",
    "    for example in D:\n",
    "        example_class = int(example[\"label\"])  # Get the class label for the example\n",
    "        cls_prior[example_class] += 1  # Count the number of documents for each class\n",
    "        for word in example[\"tokens\"].split(\"|\"):  # Tokenize the example\n",
    "            # Update word count for the corresponding class\n",
    "            cls_word_cnt[example_class][word] = cls_word_cnt[example_class].get(word, 0) + 1\n",
    "            vocab.add(word)  # Add word to the vocabulary\n",
    "           \n",
    "    cls_prior /= n_doc  # Calculate prior probabilities\n",
    "    log_prior = np.log(cls_prior)  # Convert prior probabilities to log space\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    cls_total_word_cnt = np.array([sum(cls_dict.values()) for cls_dict in cls_word_cnt])  # Total word count for each class\n",
    "    \n",
    "    # Calculate log likelihood with smoothing (Laplace smoothing)\n",
    "    log_likelihood = np.array([\n",
    "        {word: np.log((cls_dict.get(word,0) + 1) / (cls_total_word_cnt[i] + vocab_size)) for word in vocab} \n",
    "        for i, cls_dict in enumerate(cls_word_cnt)\n",
    "    ])\n",
    "    \n",
    "    return log_prior, log_likelihood, vocab\n",
    "\n",
    "def test_naive_bayes(test_doc,log_prior, log_likelihood,num_classes,vocab):\n",
    "    sum_lg = np.zeros(num_classes)\n",
    "    for c in range(num_classes):\n",
    "        sum_lg[c] = log_prior[c]\n",
    "        for word in test_doc.split():\n",
    "            if word in vocab:\n",
    "                sum_lg[c] += log_likelihood[c][word]\n",
    "                \n",
    "    return np.argmax(sum_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior, log_likelihood, vocab = train_naive_bayes(train_data,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for doc in test_data:\n",
    "    predictions.append(test_naive_bayes(doc[\"sentence\"],log_prior, log_likelihood, num_classes, vocab))\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = np.sum(predictions == test_labels)\n",
    "    \n",
    "# Calculate accuracy\n",
    "my_accuracy = correct_predictions / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3995475113122172\n"
     ]
    }
   ],
   "source": [
    "print(my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40497737556561086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(train_data[\"sentence\"], train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(test_data[\"sentence\"])\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_cnt(D):\n",
    "    \n",
    "    bigrams = {}\n",
    "    for example in D:\n",
    "        words_list = example[\"tokens\"].split(\"|\")\n",
    "        for i in range(len(words_list)-1):\n",
    "            bigram = words_list[i] + \" \" + words_list[i+1]\n",
    "            if bigram not in bigrams:\n",
    "                bigrams[bigram] = len(bigrams)\n",
    "    \n",
    "    return bigrams\n",
    "                \n",
    "def generate_feat(D,bigrams,num_classes):\n",
    "    test_data_feat = np.zeros((len(D),len(bigrams)+1),dtype=np.int8)\n",
    "    test_data_labels = np.zeros((len(D),num_classes),dtype=np.int8)\n",
    "    for i,example in enumerate(D):\n",
    "        test_data_labels[i][int(example[\"label\"])] = 1\n",
    "        words_list = example[\"tokens\"].split(\"|\")\n",
    "        for j in range(len(words_list)-1):\n",
    "            bigram = words_list[j] + \" \" + words_list[j+1]\n",
    "            if bigram in bigrams:\n",
    "                test_data_feat[i][bigrams[bigram]] = 1\n",
    "            \n",
    "        \n",
    "    return test_data_feat ,test_data_labels     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9645, 5)\n",
      "9645\n",
      "[[0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bigrams = get_bigrams_cnt(train_data)\n",
    "X,Y = generate_feat(train_data,bigrams,num_classes)\n",
    "print(Y.shape)\n",
    "print(len(X))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X, Y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:min(i + batch_size,len(X))]\n",
    "        batch_Y = Y[i:min(i + batch_size,len(X))]\n",
    "        yield batch_X, batch_Y  # Yielding both data and labels\n",
    "\n",
    "def softmax(X:np.ndarray,W:np.ndarray,B:np.ndarray):  \n",
    "    W_exp = np.exp(X @ W + B )   #N*f x f*c = N*c\n",
    "    W_exp_sum = np.sum(W_exp,axis = 1).reshape(-1,1)\n",
    "\n",
    "    return W_exp/W_exp_sum\n",
    "\n",
    "def loss(X:np.ndarray,Y:np.ndarray,W:np.ndarray,B:np.ndarray):\n",
    "    soft_max = softmax(X,W,B)\n",
    "    req_ind = np.argmax(Y, axis=1)  # Row indices of non-zero elements\n",
    "    req_values = np.log(soft_max[np.arange(soft_max.shape[0]),req_ind])\n",
    "   \n",
    "    return -1/X.shape[0]*np.sum(req_values)\n",
    "    \n",
    "def calculate_gradient(X:np.ndarray,Y:np.ndarray,W:np.ndarray,B:np.ndarray,num_classes,mu = 0.01):\n",
    "    derivative = Y * (Y - softmax(X,W,B))\n",
    "    \n",
    "    # Identify the indices of non-zero entries in each column\n",
    "    non_zero_cols = np.argmax(Y, axis=1)  # cols indices of non-zero elements\n",
    "    non_zero_values = derivative[np.arange(derivative.shape[0]),non_zero_cols]  # Non-zero values in derivative\n",
    "    \n",
    "    # Vectorized calculation of dW\n",
    "    dW = np.zeros((W.shape[1],W.shape[0]))\n",
    "    np.add.at(dW, non_zero_cols, non_zero_values[:, np.newaxis] * X)  # Efficient col-wise update\n",
    "    dW = -1/X.shape[0] * dW.T\n",
    "    #dW += 2 * mu * W   #regularization term\n",
    "    \n",
    "    # Calculate dB directly\n",
    "    dB = -1/X.shape[0]*np.sum(derivative, axis=0)\n",
    "    \n",
    "    return  dW, dB.reshape(1, num_classes)\n",
    "          \n",
    "def logistic_regression(X:np.ndarray,Y:np.ndarray,num_classes,lr=0.0001,max_itr=100,batch_size=15):\n",
    "    num_examples, num_features = X.shape\n",
    "    W = np.zeros((num_features,num_classes))\n",
    "    B = np.zeros((1,num_classes))\n",
    "    itr = 0\n",
    "\n",
    "    while itr < max_itr:\n",
    "        print(f\"itr{itr+1}:\")\n",
    "        print(\"=============================================================================\")\n",
    "        for batch_X, batch_Y in create_batches(X, Y, batch_size):\n",
    "            \n",
    "            dW, dB = calculate_gradient(batch_X,batch_Y,W,B,num_classes)  \n",
    "            W -= lr * dW\n",
    "            B -= lr * dB\n",
    "        \n",
    "            print(f\"The loss = {loss(batch_X,batch_Y,W,B)}\")\n",
    "        \n",
    "        print(\"=============================================================================\")\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X,Y,num_classes,lr=0.01,max_itr=10,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scikit learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
