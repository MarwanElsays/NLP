{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset,concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('sst', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 0.625,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class(value):\n",
    "    return np.digitize(value, bins=[0.2, 0.4, 0.6, 0.8], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapped = dataset.map(lambda example: {'label': map_class(example['label'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training set\n",
    "train_data = concatenate_datasets([dataset_mapped['train'], dataset_mapped['validation']])\n",
    "train_labels = np.array(train_data['label'],dtype=np.int32)\n",
    "\n",
    "# Access the test set\n",
    "test_data = dataset_mapped['test']\n",
    "test_labels = np.array(test_data['label'],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 3.0,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = test_data[10]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Na√Øve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D,num_classes):\n",
    "     \n",
    "    n_doc = len(D)  # Total number of documents/examples\n",
    "    cls_prior = np.zeros(num_classes)  # Initialize class prior probabilities\n",
    "    vocab = set()  # To store the vocabulary\n",
    "    cls_word_cnt = np.array([{} for _ in range(num_classes)])  # Initialize class word counts\n",
    "\n",
    "    # Loop through each example in the dataset\n",
    "    for example in D:\n",
    "        example_class = int(example[\"label\"])  # Get the class label for the example\n",
    "        cls_prior[example_class] += 1  # Count the number of documents for each class\n",
    "        for word in example[\"tokens\"].split(\"|\"):  # Tokenize the example\n",
    "            # Update word count for the corresponding class\n",
    "            cls_word_cnt[example_class][word] = cls_word_cnt[example_class].get(word, 0) + 1\n",
    "            vocab.add(word)  # Add word to the vocabulary\n",
    "           \n",
    "    cls_prior /= n_doc  # Calculate prior probabilities\n",
    "    log_prior = np.log(cls_prior)  # Convert prior probabilities to log space\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    cls_total_word_cnt = np.array([sum(cls_dict.values()) for cls_dict in cls_word_cnt])  # Total word count for each class\n",
    "    \n",
    "    # Calculate log likelihood with smoothing (Laplace smoothing)\n",
    "    log_likelihood = np.array([\n",
    "        {word: np.log((cls_dict.get(word,0) + 1) / (cls_total_word_cnt[i] + vocab_size)) for word in vocab} \n",
    "        for i, cls_dict in enumerate(cls_word_cnt)\n",
    "    ])\n",
    "    \n",
    "    return log_prior, log_likelihood, vocab\n",
    "\n",
    "def test_naive_bayes(test_doc,log_prior, log_likelihood,num_classes,vocab):\n",
    "    sum_lg = np.zeros(num_classes)\n",
    "    for c in range(num_classes):\n",
    "        sum_lg[c] = log_prior[c]\n",
    "        for word in test_doc.split():\n",
    "            if word in vocab:\n",
    "                sum_lg[c] += log_likelihood[c][word]\n",
    "                \n",
    "    return np.argmax(sum_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "log_prior, log_likelihood, vocab = train_naive_bayes(train_data,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for doc in test_data:\n",
    "    predictions.append(test_naive_bayes(doc[\"sentence\"],log_prior, log_likelihood, num_classes, vocab))\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = np.sum(predictions == test_labels)\n",
    "    \n",
    "# Calculate accuracy\n",
    "my_accuracy = correct_predictions / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3995475113122172\n"
     ]
    }
   ],
   "source": [
    "print(my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40497737556561086\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.09      0.15       279\n",
      "           1       0.46      0.65      0.54       633\n",
      "           2       0.21      0.09      0.13       389\n",
      "           3       0.36      0.65      0.46       510\n",
      "           4       0.58      0.23      0.33       399\n",
      "\n",
      "    accuracy                           0.40      2210\n",
      "   macro avg       0.40      0.34      0.32      2210\n",
      "weighted avg       0.41      0.40      0.36      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(train_data[\"sentence\"], train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(test_data[\"sentence\"])\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
