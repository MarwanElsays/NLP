{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset,concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('sst', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 0.625,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class(value):\n",
    "    return np.digitize(value, bins=[0.2, 0.4, 0.6, 0.8], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapped = dataset.map(lambda example: {'label': map_class(example['label'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training set\n",
    "train_data = concatenate_datasets([dataset_mapped['train'], dataset_mapped['validation']])\n",
    "train_labels = np.array(train_data['label'],dtype=np.int8)\n",
    "\n",
    "# Access the test set\n",
    "test_data = dataset_mapped['test']\n",
    "test_labels = np.array(test_data['label'],dtype=np.int8)\n",
    "\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What really surprises about Wisegirls is its low-key quality and genuine tenderness .',\n",
       " 'label': 3.0,\n",
       " 'tokens': 'What|really|surprises|about|Wisegirls|is|its|low-key|quality|and|genuine|tenderness|.',\n",
       " 'tree': '25|24|22|21|21|19|16|15|15|17|14|14|20|18|16|17|18|19|20|23|22|23|24|25|0'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = test_data[10]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Na√Øve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D,num_classes):\n",
    "     \n",
    "    n_doc = len(D)  # Total number of documents/examples\n",
    "    cls_prior = np.zeros(num_classes)  # Initialize class prior probabilities\n",
    "    vocab = set()  # To store the vocabulary\n",
    "    cls_word_cnt = np.array([{} for _ in range(num_classes)])  # Initialize class word counts\n",
    "\n",
    "    # Loop through each example in the dataset\n",
    "    for example in D:\n",
    "        example_class = int(example[\"label\"])  # Get the class label for the example\n",
    "        cls_prior[example_class] += 1  # Count the number of documents for each class\n",
    "        for word in example[\"tokens\"].split(\"|\"):  # Tokenize the example\n",
    "            # Update word count for the corresponding class\n",
    "            cls_word_cnt[example_class][word] = cls_word_cnt[example_class].get(word, 0) + 1\n",
    "            vocab.add(word)  # Add word to the vocabulary\n",
    "           \n",
    "    cls_prior /= n_doc  # Calculate prior probabilities\n",
    "    log_prior = np.log(cls_prior)  # Convert prior probabilities to log space\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    cls_total_word_cnt = np.array([sum(cls_dict.values()) for cls_dict in cls_word_cnt])  # Total word count for each class\n",
    "    \n",
    "    # Calculate log likelihood with smoothing (Laplace smoothing)\n",
    "    log_likelihood = np.array([\n",
    "        {word: np.log((cls_dict.get(word,0) + 1) / (cls_total_word_cnt[i] + vocab_size)) for word in vocab} \n",
    "        for i, cls_dict in enumerate(cls_word_cnt)\n",
    "    ])\n",
    "    \n",
    "    return log_prior, log_likelihood, vocab\n",
    "\n",
    "def test_naive_bayes(test_doc,log_prior, log_likelihood,num_classes,vocab):\n",
    "    sum_lg = np.zeros(num_classes)\n",
    "    for c in range(num_classes):\n",
    "        sum_lg[c] = log_prior[c]\n",
    "        for word in test_doc.split():\n",
    "            if word in vocab:\n",
    "                sum_lg[c] += log_likelihood[c][word]\n",
    "                \n",
    "    return np.argmax(sum_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior, log_likelihood, vocab = train_naive_bayes(train_data,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for doc in test_data:\n",
    "    predictions.append(test_naive_bayes(doc[\"sentence\"],log_prior, log_likelihood, num_classes, vocab))\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = np.sum(predictions == test_labels)\n",
    "    \n",
    "# Calculate accuracy\n",
    "my_accuracy = correct_predictions / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3995475113122172\n"
     ]
    }
   ],
   "source": [
    "print(my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40497737556561086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(train_data[\"sentence\"], train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(test_data[\"sentence\"])\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_cnt(D):\n",
    "    \n",
    "    bigrams = {}\n",
    "    for example in D:\n",
    "        words_list = example[\"tokens\"].split(\"|\")\n",
    "        for i in range(len(words_list)-1):\n",
    "            bigram = words_list[i] + \" \" + words_list[i+1]\n",
    "            if bigram not in bigrams:\n",
    "                bigrams[bigram] = len(bigrams)\n",
    "    \n",
    "    return bigrams\n",
    "                \n",
    "def generate_feat(D,bigrams,num_classes):\n",
    "    test_data_feat = np.zeros((len(D),len(bigrams)+1),dtype=np.int8)\n",
    "    test_data_labels = np.zeros((len(D),num_classes),dtype=np.int8)\n",
    "    for i,example in enumerate(D):\n",
    "        test_data_labels[i][int(example[\"label\"])] = 1\n",
    "        words_list = example[\"tokens\"].split(\"|\")\n",
    "        for j in range(len(words_list)-1):\n",
    "            bigram = words_list[j] + \" \" + words_list[j+1]\n",
    "            if bigram in bigrams:\n",
    "                test_data_feat[i][bigrams[bigram]] = 1\n",
    "            \n",
    "        \n",
    "    return test_data_feat ,test_data_labels     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9645, 5)\n",
      "9645\n",
      "[[0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bigrams = get_bigrams_cnt(train_data)\n",
    "X,Y = generate_feat(train_data,bigrams,num_classes)\n",
    "print(Y.shape)\n",
    "print(len(X))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X, Y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:min(i + batch_size,len(X))]\n",
    "        batch_Y = Y[i:min(i + batch_size,len(X))]\n",
    "        yield batch_X, batch_Y\n",
    "\n",
    "def softmax(X:np.ndarray,W:np.ndarray,B:np.ndarray):  \n",
    "    W_exp = np.exp(X @ W + B )   #N*f x f*c = N*c\n",
    "    W_exp_sum = np.sum(W_exp,axis = 1).reshape(-1,1)\n",
    "\n",
    "    return W_exp/W_exp_sum\n",
    "\n",
    "def loss(X:np.ndarray,Y:np.ndarray,W:np.ndarray,B:np.ndarray):\n",
    "    probabilities = softmax(X, W, B)\n",
    "    log_likelihood = -np.log(np.sum(Y * probabilities, axis=1))\n",
    "    return np.mean(log_likelihood)\n",
    "    \n",
    "def calculate_gradient(X:np.ndarray,Y:np.ndarray,W:np.ndarray,B:np.ndarray):\n",
    "    p = Y - softmax(X, W, B)\n",
    "    dW = - (X.T @ p) / X.shape[0]\n",
    "    dB = - np.sum(p, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    return  dW,dB\n",
    "          \n",
    "def logistic_regression(X: np.ndarray, Y: np.ndarray, num_classes, \n",
    "                        lr=0.001, max_itr=100, batch_size=15, \n",
    "                        decay_factor=1, step_size=100,loaded_W = None,loaded_B = None):\n",
    "    \n",
    "    _, num_features = X.shape\n",
    "    \n",
    "    W = np.zeros((num_features,num_classes))\n",
    "    B = np.zeros((1,num_classes))\n",
    "    if loaded_W is not None and loaded_B is not None:\n",
    "        W = loaded_W\n",
    "        B = loaded_B\n",
    " \n",
    "    # Initialize variables to store the best loss and model parameters\n",
    "    best_loss = float('inf')\n",
    "    W_best = None\n",
    "    B_best = None\n",
    "    itr = 0\n",
    "    \n",
    "    for itr in range(max_itr):\n",
    "        print(f\"Iteration {itr+1}:\")\n",
    "        print(\"=============================================================================\")\n",
    "        \n",
    "        epoch_loss = 0  # Initialize the loss for the entire epoch\n",
    "        num_batches = 0  # Counter for the number of batches processed\n",
    "        \n",
    "        for batch_X, batch_Y in create_batches(X, Y, batch_size):\n",
    "            # Calculate gradients\n",
    "            dW, dB = calculate_gradient(batch_X, batch_Y, W, B)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            W -= lr * dW\n",
    "            B -= lr * dB\n",
    "            \n",
    "            # Calculate the batch loss\n",
    "            batch_loss = loss(batch_X, batch_Y, W, B)\n",
    "            epoch_loss += batch_loss\n",
    "            num_batches += 1  # Increment batch counter\n",
    "        \n",
    "        # Calculate average loss over all batches\n",
    "        total_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Update the best loss if current epoch loss is lower\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            W_best = W.copy()\n",
    "            B_best = B.copy()\n",
    "            \n",
    "        # Step decay of learning rate\n",
    "        if (itr + 1) % step_size == 0:  # If we've completed a step\n",
    "            lr *= decay_factor  # Reduce the learning rate\n",
    "        \n",
    "        print(f\"The current loss for iteration {itr+1} = {total_loss}\")\n",
    "        print(\"=============================================================================\")\n",
    "    \n",
    "    print(\"Best Loss:\", best_loss)\n",
    "    \n",
    "    return W, B, W_best, B_best\n",
    "\n",
    "def predict(X,W,B):\n",
    "    return np.argmax(softmax(X,W,B),axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "=============================================================================\n",
      "The current loss for iteration 1 = 0.4220532272236317\n",
      "=============================================================================\n",
      "Iteration 2:\n",
      "=============================================================================\n",
      "The current loss for iteration 2 = 0.28308674918788773\n",
      "=============================================================================\n",
      "Iteration 3:\n",
      "=============================================================================\n",
      "The current loss for iteration 3 = 0.21527912156665888\n",
      "=============================================================================\n",
      "Iteration 4:\n",
      "=============================================================================\n",
      "The current loss for iteration 4 = 0.1742661083574722\n",
      "=============================================================================\n",
      "Iteration 5:\n",
      "=============================================================================\n",
      "The current loss for iteration 5 = 0.14673007498203658\n",
      "=============================================================================\n",
      "Iteration 6:\n",
      "=============================================================================\n",
      "The current loss for iteration 6 = 0.12694858431703945\n",
      "=============================================================================\n",
      "Iteration 7:\n",
      "=============================================================================\n",
      "The current loss for iteration 7 = 0.11202580287704016\n",
      "=============================================================================\n",
      "Iteration 8:\n",
      "=============================================================================\n",
      "The current loss for iteration 8 = 0.10034809263323118\n",
      "=============================================================================\n",
      "Iteration 9:\n",
      "=============================================================================\n",
      "The current loss for iteration 9 = 0.09094417335552755\n",
      "=============================================================================\n",
      "Iteration 10:\n",
      "=============================================================================\n",
      "The current loss for iteration 10 = 0.08319602820774731\n",
      "=============================================================================\n",
      "Iteration 11:\n",
      "=============================================================================\n",
      "The current loss for iteration 11 = 0.07669380615301023\n",
      "=============================================================================\n",
      "Iteration 12:\n",
      "=============================================================================\n",
      "The current loss for iteration 12 = 0.07115545376468878\n",
      "=============================================================================\n",
      "Iteration 13:\n",
      "=============================================================================\n",
      "The current loss for iteration 13 = 0.0663799531953129\n",
      "=============================================================================\n",
      "Iteration 14:\n",
      "=============================================================================\n",
      "The current loss for iteration 14 = 0.06221951522874426\n",
      "=============================================================================\n",
      "Iteration 15:\n",
      "=============================================================================\n",
      "The current loss for iteration 15 = 0.058562619931056756\n",
      "=============================================================================\n",
      "Iteration 16:\n",
      "=============================================================================\n",
      "The current loss for iteration 16 = 0.05532333079794334\n",
      "=============================================================================\n",
      "Iteration 17:\n",
      "=============================================================================\n",
      "The current loss for iteration 17 = 0.052434298484301374\n",
      "=============================================================================\n",
      "Iteration 18:\n",
      "=============================================================================\n",
      "The current loss for iteration 18 = 0.04984198252251084\n",
      "=============================================================================\n",
      "Iteration 19:\n",
      "=============================================================================\n",
      "The current loss for iteration 19 = 0.047503269409322764\n",
      "=============================================================================\n",
      "Iteration 20:\n",
      "=============================================================================\n",
      "The current loss for iteration 20 = 0.04538302701201838\n",
      "=============================================================================\n",
      "Iteration 21:\n",
      "=============================================================================\n",
      "The current loss for iteration 21 = 0.04345231466463787\n",
      "=============================================================================\n",
      "Iteration 22:\n",
      "=============================================================================\n",
      "The current loss for iteration 22 = 0.04168705873354994\n",
      "=============================================================================\n",
      "Iteration 23:\n",
      "=============================================================================\n",
      "The current loss for iteration 23 = 0.04006705941649486\n",
      "=============================================================================\n",
      "Iteration 24:\n",
      "=============================================================================\n",
      "The current loss for iteration 24 = 0.03857523486105253\n",
      "=============================================================================\n",
      "Iteration 25:\n",
      "=============================================================================\n",
      "The current loss for iteration 25 = 0.03719703774237651\n",
      "=============================================================================\n",
      "Iteration 26:\n",
      "=============================================================================\n",
      "The current loss for iteration 26 = 0.03591999940571009\n",
      "=============================================================================\n",
      "Iteration 27:\n",
      "=============================================================================\n",
      "The current loss for iteration 27 = 0.034733369969936125\n",
      "=============================================================================\n",
      "Iteration 28:\n",
      "=============================================================================\n",
      "The current loss for iteration 28 = 0.033627831615945694\n",
      "=============================================================================\n",
      "Iteration 29:\n",
      "=============================================================================\n",
      "The current loss for iteration 29 = 0.0325952682762211\n",
      "=============================================================================\n",
      "Iteration 30:\n",
      "=============================================================================\n",
      "The current loss for iteration 30 = 0.03162857914778924\n",
      "=============================================================================\n",
      "Iteration 31:\n",
      "=============================================================================\n",
      "The current loss for iteration 31 = 0.03072152649838404\n",
      "=============================================================================\n",
      "Iteration 32:\n",
      "=============================================================================\n",
      "The current loss for iteration 32 = 0.029868610495970678\n",
      "=============================================================================\n",
      "Iteration 33:\n",
      "=============================================================================\n",
      "The current loss for iteration 33 = 0.029064965490099934\n",
      "=============================================================================\n",
      "Iteration 34:\n",
      "=============================================================================\n",
      "The current loss for iteration 34 = 0.028306273456404317\n",
      "=============================================================================\n",
      "Iteration 35:\n",
      "=============================================================================\n",
      "The current loss for iteration 35 = 0.027588691285699775\n",
      "=============================================================================\n",
      "Iteration 36:\n",
      "=============================================================================\n",
      "The current loss for iteration 36 = 0.026908789332893233\n",
      "=============================================================================\n",
      "Iteration 37:\n",
      "=============================================================================\n",
      "The current loss for iteration 37 = 0.026263499196537064\n",
      "=============================================================================\n",
      "Iteration 38:\n",
      "=============================================================================\n",
      "The current loss for iteration 38 = 0.02565006912212851\n",
      "=============================================================================\n",
      "Iteration 39:\n",
      "=============================================================================\n",
      "The current loss for iteration 39 = 0.02506602574508248\n",
      "=============================================================================\n",
      "Iteration 40:\n",
      "=============================================================================\n",
      "The current loss for iteration 40 = 0.02450914113817596\n",
      "=============================================================================\n",
      "Iteration 41:\n",
      "=============================================================================\n",
      "The current loss for iteration 41 = 0.023977404321960792\n",
      "=============================================================================\n",
      "Iteration 42:\n",
      "=============================================================================\n",
      "The current loss for iteration 42 = 0.023468996548954017\n",
      "=============================================================================\n",
      "Iteration 43:\n",
      "=============================================================================\n",
      "The current loss for iteration 43 = 0.02298226979335966\n",
      "=============================================================================\n",
      "Iteration 44:\n",
      "=============================================================================\n",
      "The current loss for iteration 44 = 0.022515727975012403\n",
      "=============================================================================\n",
      "Iteration 45:\n",
      "=============================================================================\n",
      "The current loss for iteration 45 = 0.02206801052454999\n",
      "=============================================================================\n",
      "Iteration 46:\n",
      "=============================================================================\n",
      "The current loss for iteration 46 = 0.021637877960545546\n",
      "=============================================================================\n",
      "Iteration 47:\n",
      "=============================================================================\n",
      "The current loss for iteration 47 = 0.02122419920150233\n",
      "=============================================================================\n",
      "Iteration 48:\n",
      "=============================================================================\n",
      "The current loss for iteration 48 = 0.020825940378548964\n",
      "=============================================================================\n",
      "Iteration 49:\n",
      "=============================================================================\n",
      "The current loss for iteration 49 = 0.020442154950182735\n",
      "=============================================================================\n",
      "Iteration 50:\n",
      "=============================================================================\n",
      "The current loss for iteration 50 = 0.020071974949902913\n",
      "=============================================================================\n",
      "Iteration 51:\n",
      "=============================================================================\n",
      "The current loss for iteration 51 = 0.019714603222173246\n",
      "=============================================================================\n",
      "Iteration 52:\n",
      "=============================================================================\n",
      "The current loss for iteration 52 = 0.01936930652275421\n",
      "=============================================================================\n",
      "Iteration 53:\n",
      "=============================================================================\n",
      "The current loss for iteration 53 = 0.019035409376761767\n",
      "=============================================================================\n",
      "Iteration 54:\n",
      "=============================================================================\n",
      "The current loss for iteration 54 = 0.01871228860241945\n",
      "=============================================================================\n",
      "Iteration 55:\n",
      "=============================================================================\n",
      "The current loss for iteration 55 = 0.018399368420851995\n",
      "=============================================================================\n",
      "Iteration 56:\n",
      "=============================================================================\n",
      "The current loss for iteration 56 = 0.01809611608278855\n",
      "=============================================================================\n",
      "Iteration 57:\n",
      "=============================================================================\n",
      "The current loss for iteration 57 = 0.017802037952019543\n",
      "=============================================================================\n",
      "Iteration 58:\n",
      "=============================================================================\n",
      "The current loss for iteration 58 = 0.017516675993138664\n",
      "=============================================================================\n",
      "Iteration 59:\n",
      "=============================================================================\n",
      "The current loss for iteration 59 = 0.017239604617698287\n",
      "=============================================================================\n",
      "Iteration 60:\n",
      "=============================================================================\n",
      "The current loss for iteration 60 = 0.01697042784859051\n",
      "=============================================================================\n",
      "Iteration 61:\n",
      "=============================================================================\n",
      "The current loss for iteration 61 = 0.016708776767383925\n",
      "=============================================================================\n",
      "Iteration 62:\n",
      "=============================================================================\n",
      "The current loss for iteration 62 = 0.01645430721358855\n",
      "=============================================================================\n",
      "Iteration 63:\n",
      "=============================================================================\n",
      "The current loss for iteration 63 = 0.016206697708524676\n",
      "=============================================================================\n",
      "Iteration 64:\n",
      "=============================================================================\n",
      "The current loss for iteration 64 = 0.015965647579682465\n",
      "=============================================================================\n",
      "Iteration 65:\n",
      "=============================================================================\n",
      "The current loss for iteration 65 = 0.015730875264259498\n",
      "=============================================================================\n",
      "Iteration 66:\n",
      "=============================================================================\n",
      "The current loss for iteration 66 = 0.015502116773018253\n",
      "=============================================================================\n",
      "Iteration 67:\n",
      "=============================================================================\n",
      "The current loss for iteration 67 = 0.015279124297747478\n",
      "=============================================================================\n",
      "Iteration 68:\n",
      "=============================================================================\n",
      "The current loss for iteration 68 = 0.015061664947492975\n",
      "=============================================================================\n",
      "Iteration 69:\n",
      "=============================================================================\n",
      "The current loss for iteration 69 = 0.01484951960037575\n",
      "=============================================================================\n",
      "Iteration 70:\n",
      "=============================================================================\n",
      "The current loss for iteration 70 = 0.014642481859271788\n",
      "=============================================================================\n",
      "Iteration 71:\n",
      "=============================================================================\n",
      "The current loss for iteration 71 = 0.014440357100905526\n",
      "=============================================================================\n",
      "Iteration 72:\n",
      "=============================================================================\n",
      "The current loss for iteration 72 = 0.014242961609045706\n",
      "=============================================================================\n",
      "Iteration 73:\n",
      "=============================================================================\n",
      "The current loss for iteration 73 = 0.014050121783485216\n",
      "=============================================================================\n",
      "Iteration 74:\n",
      "=============================================================================\n",
      "The current loss for iteration 74 = 0.013861673417379998\n",
      "=============================================================================\n",
      "Iteration 75:\n",
      "=============================================================================\n",
      "The current loss for iteration 75 = 0.01367746103629896\n",
      "=============================================================================\n",
      "Iteration 76:\n",
      "=============================================================================\n",
      "The current loss for iteration 76 = 0.013497337293037039\n",
      "=============================================================================\n",
      "Iteration 77:\n",
      "=============================================================================\n",
      "The current loss for iteration 77 = 0.013321162412862488\n",
      "=============================================================================\n",
      "Iteration 78:\n",
      "=============================================================================\n",
      "The current loss for iteration 78 = 0.013148803684416994\n",
      "=============================================================================\n",
      "Iteration 79:\n",
      "=============================================================================\n",
      "The current loss for iteration 79 = 0.012980134991980773\n",
      "=============================================================================\n",
      "Iteration 80:\n",
      "=============================================================================\n",
      "The current loss for iteration 80 = 0.01281503638524834\n",
      "=============================================================================\n",
      "Iteration 81:\n",
      "=============================================================================\n",
      "The current loss for iteration 81 = 0.012653393683154102\n",
      "=============================================================================\n",
      "Iteration 82:\n",
      "=============================================================================\n",
      "The current loss for iteration 82 = 0.012495098108631173\n",
      "=============================================================================\n",
      "Iteration 83:\n",
      "=============================================================================\n",
      "The current loss for iteration 83 = 0.012340045951503571\n",
      "=============================================================================\n",
      "Iteration 84:\n",
      "=============================================================================\n",
      "The current loss for iteration 84 = 0.012188138256984098\n",
      "=============================================================================\n",
      "Iteration 85:\n",
      "=============================================================================\n",
      "The current loss for iteration 85 = 0.012039280537505765\n",
      "=============================================================================\n",
      "Iteration 86:\n",
      "=============================================================================\n",
      "The current loss for iteration 86 = 0.011893382505834452\n",
      "=============================================================================\n",
      "Iteration 87:\n",
      "=============================================================================\n",
      "The current loss for iteration 87 = 0.011750357827610807\n",
      "=============================================================================\n",
      "Iteration 88:\n",
      "=============================================================================\n",
      "The current loss for iteration 88 = 0.011610123891653034\n",
      "=============================================================================\n",
      "Iteration 89:\n",
      "=============================================================================\n",
      "The current loss for iteration 89 = 0.011472601596506843\n",
      "=============================================================================\n",
      "Iteration 90:\n",
      "=============================================================================\n",
      "The current loss for iteration 90 = 0.011337715151881176\n",
      "=============================================================================\n",
      "Iteration 91:\n",
      "=============================================================================\n",
      "The current loss for iteration 91 = 0.011205391893732796\n",
      "=============================================================================\n",
      "Iteration 92:\n",
      "=============================================================================\n",
      "The current loss for iteration 92 = 0.011075562111881616\n",
      "=============================================================================\n",
      "Iteration 93:\n",
      "=============================================================================\n",
      "The current loss for iteration 93 = 0.010948158889145174\n",
      "=============================================================================\n",
      "Iteration 94:\n",
      "=============================================================================\n",
      "The current loss for iteration 94 = 0.010823117951073321\n",
      "=============================================================================\n",
      "Iteration 95:\n",
      "=============================================================================\n",
      "The current loss for iteration 95 = 0.010700377525448402\n",
      "=============================================================================\n",
      "Iteration 96:\n",
      "=============================================================================\n",
      "The current loss for iteration 96 = 0.010579878210797342\n",
      "=============================================================================\n",
      "Iteration 97:\n",
      "=============================================================================\n",
      "The current loss for iteration 97 = 0.01046156285322429\n",
      "=============================================================================\n",
      "Iteration 98:\n",
      "=============================================================================\n",
      "The current loss for iteration 98 = 0.010345376430939706\n",
      "=============================================================================\n",
      "Iteration 99:\n",
      "=============================================================================\n",
      "The current loss for iteration 99 = 0.010231265945915877\n",
      "=============================================================================\n",
      "Iteration 100:\n",
      "=============================================================================\n",
      "The current loss for iteration 100 = 0.010119180322148437\n",
      "=============================================================================\n",
      "Best Loss: 0.010119180322148437\n"
     ]
    }
   ],
   "source": [
    "W, B, W_best, B_best = logistic_regression(X,Y,num_classes,lr=0.1,max_itr=100,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test = generate_feat(test_data,bigrams,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34570135746606334"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = predict(X_test,W_best, B_best)\n",
    "\n",
    "accuracy = np.sum(pred_labels == test_labels)/len(test_labels)\n",
    "accuracy\n",
    "# loss(X,Y,best_W,best_B)\n",
    "# loss(X,Y,W,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"last_model.npz\", W=W, B=B)\n",
    "np.savez(\"best_model.npz\", W=W_best, B=B_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3556561085972851"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .npz file\n",
    "data = np.load(\"best_model.npz\")\n",
    "\n",
    "# Access the W and B matrices\n",
    "W = data[\"W\"]\n",
    "B = data[\"B\"]\n",
    "\n",
    "pred = predict(X_test,W,B)\n",
    "\n",
    "np.sum(pred == test_labels)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "=============================================================================\n",
      "The current loss for iteration 1 = 0.2638735078212166\n",
      "=============================================================================\n",
      "Iteration 2:\n",
      "=============================================================================\n",
      "The current loss for iteration 2 = 0.24669123679043295\n",
      "=============================================================================\n",
      "Iteration 3:\n",
      "=============================================================================\n",
      "The current loss for iteration 3 = 0.2292699186075083\n",
      "=============================================================================\n",
      "Iteration 4:\n",
      "=============================================================================\n",
      "The current loss for iteration 4 = 0.21423290831714575\n",
      "=============================================================================\n",
      "Iteration 5:\n",
      "=============================================================================\n",
      "The current loss for iteration 5 = 0.20110032997485092\n",
      "=============================================================================\n",
      "Iteration 6:\n",
      "=============================================================================\n",
      "The current loss for iteration 6 = 0.18952305003514133\n",
      "=============================================================================\n",
      "Iteration 7:\n",
      "=============================================================================\n",
      "The current loss for iteration 7 = 0.1792358037750135\n",
      "=============================================================================\n",
      "Iteration 8:\n",
      "=============================================================================\n",
      "The current loss for iteration 8 = 0.17003187309234524\n",
      "=============================================================================\n",
      "Iteration 9:\n",
      "=============================================================================\n",
      "The current loss for iteration 9 = 0.1617471501482859\n",
      "=============================================================================\n",
      "Iteration 10:\n",
      "=============================================================================\n",
      "The current loss for iteration 10 = 0.15424940534230058\n",
      "=============================================================================\n",
      "Best Loss: 0.15424940534230058\n"
     ]
    }
   ],
   "source": [
    "W, B, W_best, B_best = logistic_regression(X,Y,num_classes,lr=1,max_itr=5,batch_size=1,loaded_W=W,loaded_B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3389140271493213"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = predict(X_test,W_best, B_best)\n",
    "\n",
    "accuracy = np.sum(pred_labels == test_labels)/len(test_labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Initialize LogisticRegression\n",
    "clf_logreg = LogisticRegression(max_iter=100, multi_class='multinomial', solver='lbfgs')\n",
    "clf_logreg.fit(X, train_labels)  # Assuming X_train, Y_train are your training sets\n",
    "\n",
    "\n",
    "# Initialize SGDClassifier as logistic regression\n",
    "clf_sgd = SGDClassifier(loss='log_loss', max_iter=100, tol=1e-3, learning_rate='optimal', penalty='l2')\n",
    "clf_sgd.fit(X, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = clf_logreg.predict(X_test)\n",
    "y_pred_sgd = clf_sgd.predict(X_test)\n",
    "\n",
    "accuracy = np.sum(y_pred_logreg == test_labels)/len(test_labels)\n",
    "print(f\"logisitc regression accuracy = {accuracy}\")\n",
    "\n",
    "accuracy = np.sum(y_pred_sgd == test_labels)/len(test_labels)\n",
    "print(f\"SGD accuracy = {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
